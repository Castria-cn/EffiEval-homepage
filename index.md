---
layout: default
title: EffiEval â€“ Efficient & Generalizable Model Evaluation
---

## ðŸ”‘Â Key Takeaways
> - **Maximized capability coverage**: Select subsets using the *Model Utility Index (MUI)* to ensure representativeness.
> - **Training-free & decoupled from performance**: The sample selection process does not rely on model scores, avoiding evaluation bias.
> - **Cross-model transferability**: A subset selected with Llama-3.1-8B is equally applicable to 17 other models, including Qwen and Gemma.
> - **Maintain $\tau>0.9$ with only 5 % of the samples**, significantly reducing GPU evaluation costs.

<a id="abstract"></a>
## 1Â Introduction
With the rapid growth of large language models (LLMs), existing benchmarks such as MMLU and HELM require hundreds of GPU hours per evaluation, causing costs to surge. We propose **EffiEval**, a training-free benchmarking method that mitigates data redundancy while preserving evaluation reliability. EffiEval meets three criteria: **representativeness** (comprehensive capability coverage), **fairness** (selection independent of model performance), and **generalizability** (transferable across datasets and model families). Using the Model Utility Index (MUI), it adaptively selects small yet representative subsets, achieving ranking consistency (Ï„ > 0.9) with only a fraction of the data. Experiments on multiple benchmarks and LLMs confirm its efficiency, scalability, and robustness, offering a practical solution for reliable and cost-effective evaluation.

## 2 EffiEval
> **Key ideaï¼š** 

<img src="./assets/img/figures1.gif" alt="EffiEval Workflow" style="max-width:100%;height:auto;display:block;margin:0 auto;">


### 2.1Â ModelÂ UtilityÂ Index(MUI)
MUI quantifies the ratio of neurons activated by task $t$ to the total number of neurons: 

$$
\textbf{MUI}(t)=\frac{N_\text{activated}(t)}{N_\text{total}}
$$
The definition of $N_\text{activated}(t)$ follows [Cao et al.](https://arxiv.org/abs/2504.07440). For more details, please refer to the Methods section of the paper. Intuitively, $\textbf{MUI}(t)$ characterizes the capabilities invoked by the model when performing task $t$. Given a full dataset $\mathcal{T}=\{t_1,t_2,...,t_K\}$, we aim to retain a small subset $S=\{t_{i_1},t_{i_2},...,t_{i_k}\}$ ($k \ll K$) that represents the original dataset. From the perspective of capability coverage, this objective is equivalent to maximizing the MUI:
$$
S=\arg\max_{S\subseteq\mathcal{T}} \textbf{MUI}(S)=\arg\max_{S\subseteq\mathcal{T}}|\bigcup_{t\in S}N_\text{activated}(t)|
$$

### 2.2Â Greedy Maximum Coverage

The optimization objective in Section 2.1 is equivalent to solving a Maximum Coverage Problem. Since the number of neurons in LLMs is large, an exact solution is infeasible. Therefore, we employ a greedy algorithm to solve this problem. The pseudocode of the algorithm is provided in the paper as *Algorithm 1*, with a complexity of $O(k \cdot K)$ and an approximation ratio of $(1 - 1/e)$.

## 3Â Experiments

**Baselines.** We compare EffiEval with several representative selection strategies:  

- **Random Selection:** Subsets randomly sampled from the full dataset.  
- **Representation-Based Clustering:** Samples selected via $k$-means clustering on embeddings generated by OpenAI `text-embedding-3-large` (Following [Pacchiardi et al.](https://arxiv.org/abs/2409.03563))
- **SOTA Methods:** Publicly available representative subsets from tinyBenchmarks ([Polo et al.](https://arxiv.org/abs/2402.14992)) and metabench ([Kipnis et al.](https://arxiv.org/abs/2407.12844)).  

**Datasets.** We evaluate across four diverse benchmarks:  

- **GSM8K** for math reasoning,  
- **ARC-Challenge** for scientific reasoning,  
- **Hellaswag** for commonsense inference,  
- **MMLU** for general tasks.  

**Models.** 17 LLMs of varying scales are tested, including open-source (Qwen, LLaMA, Gemma) and closed-source (GPT-4o, Gemini) models, covering both high and low capability scenarios.  

**Metrics.** We quantify subset quality with:  

- **Ranking consistency:** Spearmanâ€™s correlation ($r_S$) and Kendallâ€™s $\tau$ ($r_K$) between subset and full dataset performance.  
- **Prediction fidelity:** Mean Absolute Error (MAE) of performance values.  

<table style="text-align: center;">
  <thead>
    <tr>
      <th rowspan="2">Method</th>
      <th colspan="3">GSM8K (k=100)</th>
      <th colspan="3">ARC (k=100)</th>
      <th colspan="3">Hellaswag (k=100)</th>
      <th colspan="3">MMLU (k=100)</th>
    </tr>
    <tr>
      <th>rS</th><th>rK</th><th>MAE â†“</th>
      <th>rS</th><th>rK</th><th>MAE â†“</th>
      <th>rS</th><th>rK</th><th>MAE â†“</th>
      <th>rS</th><th>rK</th><th>MAE â†“</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random</td><td>95.3</td><td>87.4</td><td>2.88</td>
      <td>95.4</td><td>86.5</td><td>2.88</td>
      <td>97.8</td><td>91.0</td><td>3.35</td>
      <td>95.7</td><td>85.8</td><td>3.59</td>
    </tr>
    <tr>
      <td>K-Means</td><td>95.0</td><td>87.0</td><td>2.76</td>
      <td>95.8</td><td>87.2</td><td>2.78</td>
      <td>98.1</td><td>91.5</td><td>3.30</td>
      <td>95.8</td><td>86.5</td><td>4.59</td>
    </tr>
    <tr>
      <td>tinyBenchmarks</td><td>89.5</td><td>79.6</td><td>2.12</td>
      <td>95.4</td><td>85.1</td><td>3.29</td>
      <td>98.3</td><td>91.2</td><td>6.78</td>
      <td>96.8</td><td>87.8</td><td>2.95</td>
    </tr>
    <tr>
      <td><b>EffiEval</b></td><td><b>99.2</b></td><td><b>95.9</b></td><td>4.07</td>
      <td><b>96.0</b></td><td><b>87.4</b></td><td><b>2.27</b></td>
      <td><b>98.3â€ </b></td><td><b>92.5â€ </b></td><td><b>3.09â€ </b></td>
      <td><b>96.9</b></td><td><b>89.1</b></td><td>3.45</td>
    </tr>
  </tbody>
</table>

For more results and settings of $k$, please refer to the main text of the paper. The experiment results demonstrate that EffiEval efficiently selects small, representative subsets that closely reflect full-dataset evaluation while reducing computational costs.

## 4Â Conclusion

In the era of large language models, evaluating performance can be costly and time-consuming. **EffiEval** is a training-free method that selects small, representative benchmark subsets while maximizing model capability coverage via the MUI. It ensures **representativeness, fairness, and generalizability**. Experiments on multiple public benchmarks show that EffiEval closely matches full-dataset evaluation while greatly reducing computational cost. Flexible and scalable, it lets users balance **evaluation efficiency and coverage**, making reliable benchmarking practical for a wide range of LLMs.


---

## ðŸ“œÂ BibTeX

```bibtex

```
